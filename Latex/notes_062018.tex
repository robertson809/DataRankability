\documentclass{article}
\usepackage{algorithm,algorithmic}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{enumerate}

\newcommand{\var}{{var\mkern1mu}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{remark}[theorem]{Remark}

\title{A Note on Rankability:\\
\emph{\large{Current Progress regarding Measure and the SVD}}}
\author{Thomas R. Cameron}
\date{June 2018}

\begin{document}
\maketitle
\abstract{
This note is an organization of the author's thoughts on the measure of rankability and the SVD.
At this point, our major result is a cheap measure of the rankability based on the eigenvalues of the covariance matrix associated with the adjacency matrix of a set of data. 
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                    				Introduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}	
In essence, the ranking of a data set is a one-dimensional compression of the data. 
Therefore, it is reasonable to suspect that the SVD will provide useful information in determining how rankable a set of data is. 
To gain some intuition for what information the SVD will provide we begin by analyzing the graphs given in Figure 5.1 of~\cite{Anderson}. 
Below is a table with the graphs considered and the singular values of the adjacency matrix associated with each graph. 

\begin{figure}[h]
\centering
\[
\left(
\begin{array}{cc}
 \text{Dominance Graph} & \{3.5,1.2,0.76,0.59,0.52,0.\} \\
 \text{Dominance with small perturbation} & \{3.4,1.3,1.,0.63,0.52,0.\} \\
 \text{Tree} & \{1.4,1.4,1.4,0.,0.,0.,0.\} \\
 \text{Completely Connected} & \{5.,1.,1.,1.,1.,1.\} \\
 \text{Cyclic} & \{1.,1.,1.,1.,1.,1.\} \\
 \text{Completely Decomposable} & \{1.,1.,1.,1.,1.,1.\} \\
\end{array}
\right)
\]
\caption{Singular Values of Extreme Cases}
\label{fig: table}
\end{figure}

In Section~\ref{sec: observations} we formalize several observations realized from the table in Figure~\ref{fig: table}.
In Section~\ref{sec: pca} we note that the singular values of the centered matrix or the eigenvalues of the covariance matrix may be more meaningful.
Furthermore, we propose a cheap rankability measure based on these values. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                    				Initial Observations
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Initial Observations}\label{sec: observations}
The first observations we make is that all rankable graphs correspond to a singular adjacency matrix. We formalize this observation in the following theorem.

\begin{theorem}\label{thm: basic_obs}
If the adjacency matrix of a graph is non-singular, then the corresponding graph must have a contradiction, i.e., there exists nodes $P_{i}$ and $P_{j}$ such that $P_{i}\rightarrow P_{j}$ and $P_{j}\rightarrow P_{i}$. 
\end{theorem}
\begin{proof}
The adjacency matrices of interest are $(0,1)$ matrices with $0$'s along the main diagonal. Therefore, there are $(n-1)$ choices to place a $1$ in the first row.
Note that $1$ must be placed on the first row for the adjacency matrix to be non-singular. 
Given the choice of a $1$ in the first row, there are $(n-2)$ choices to place a $1$ in the second row and not have a singular matrix. 
Resuming this construction we have $(n-n)$ choices to place a $1$ in the last row and have a non-singular matrix and the result follows.
\end{proof}

\begin{remark}
It follows from Theorem~\ref{thm: basic_obs} that if the smallest singular value of an adjacency matrix is positive, then the corresponding graph can be classified as unrankable. 
If we are willing to allow ties, then a non-singular adjacency matrix may indicate a cluster or group or equivalence class of objects that should be rated the same. 
However, this does not readily imply a metric to determine how rankable or unrankable a graph is. 
\end{remark}

To gain intuition for what the SVD can tell us about the rankability of a data set, we consider the questions proposed in~\cite[Section 5.1]{Anderson}
Specifically, we consider the minimum number of changes (edges added or removed) that must be made to a graph in order to transform it into a dominance graph.
Then, given this minimum number of changes, how many rankings can be produced.
To relate this question to the SVD we make use of the following well-known result, a proof of which can be found in~\cite[Theorem 4.2.15]{Watkins}.

\begin{theorem}\label{thm: rank_k_approx}
Let $A\in\mathbb{R}^{nxm}$ with $\text{rank}(A)=r>0$. Let $A = U\sum V^{T}$ be the SVD of A, with singular values $\sigma_{1}>\sigma_{2}>\cdots>\sigma_{r}>0$.For $k=1,\ldots,r-1$, define $A_{k} = U\sum_{k}V^{T}$, where $\sum_{k}\in\mathbb{R}^{n\times m}$
is the diagonal matrix $\text{diag}\{\sigma_{1},\ldots,\sigma_{k},0,\ldots,0\}$. Then $\text{rank}(A_{k}) = k$, and
\begin{equation}\label{eq: rank_k_approx}
\sigma_{k+1}=\left\Vert A-A_{k}\right\Vert_{2}=\min\{\left\Vert A-B\right\Vert_{2}\colon~\text{rank}(B)\leq k\}.
\end{equation}
That is, of all matrices of rank $k$ or less, $A_{k}$ is closest to A.
\end{theorem}

When changing the graph (adding or removing an edge) we are essentially considering rank $1$ perturbations of the adjacency matrix. 
Therefore, we have the following corollary.

\begin{corollary}\label{cor: rank_k_approx}
Let $A$ be the adjacency matrix of a graph with $n$ nodes and let $D$ be the adjacency matrix corresponding to a dominance graph. 
Denote by $\sigma_{1},\ldots,\sigma_{r}>0$ the singular values of $(A-D)$. Then, the graph of $A$ requires at least $r$ changes in order to translate it into the dominance graph of $D$.
\end{corollary}
\begin{proof}
Applying Theorem~\ref{thm: rank_k_approx} to the matrix $(A-D)$ we have
\[
\min\{\left\Vert (A-D)-B\right\Vert_{2}\colon~\text{rank}(B)\leq (r-1)\}>0.
\]
Thus, $(A-D)$ cannot be approximated by a rank $(r-1)$ matrix. 
Since the sum of $k$ rank $1$ matrices is at most rank $k$, it follows that at least $r$ rank $1$ perturbations must be added to $A$ in order to get $D$.
\end{proof}

\begin{remark}
As a result of Corollary~\ref{cor: rank_k_approx}, we can determine the minimum number of changes, denoted $\hat{k}$, and the corresponding $\hat{p}$.
Indeed, simply apply~\eqref{eq: rank_k_approx} to $(A-D)$ for all dominance matrices. Then apply Corollary~\ref{cor: rank_k_approx} to obtain $\hat{k}$.
Then tabulate all dominance matrices that correspond to $\hat{k}$ to obtain $\hat{p}$.
It follows that we can approximate the rankability measure given in~\cite{Anderson} by computing
\begin{equation}\label{eq: rank_meas_approx}
\frac{\hat{k}\hat{p}}{k_{max}p_{max}}.
\end{equation}
However, it is clear that this method has a factorial complexity and therefore no advantages over the algorithms discussed in~\cite{Anderson}.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                    				Connections to PCA
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Connections to PCA}\label{sec: pca}
To obtain a cheaper measure of the rankability of a dataset, we consider connections to PCA (Principal component analysis). 

The objective of ranking is to sort objects in a dataset according to some criteria. 
As a graph problem, we are finding the order or rank of vertices in a (weighted) directed graph. 
We are interested in defining a measure for the degree of rankability of a dataset. 

Let $A$ denote the adjacency matrix associated with some unweighted directed graph. 
The $j$th column of $A$, denoted $a_{j}$, shows the objects that are superior to object $j$ with respect to some criterion. 
Therefore, the column mean
\begin{equation}\label{eq: mu}
\mu = \frac{1}{n}\sum_{j=1}^{n}a_{j}
\end{equation}
is a column vector whose $i$th component is the percentage of objects that object $i$ is superior to with respect to some criterion. 

\begin{remark}
The mean is a simple measurement that delivers a good amount of information. 
The mean for each graph given in Figure 5.1 of~\cite{Anderson} is below.
\[
\left(
\begin{array}{cc}
 \text{Dominance Graph} & \{0.83,0.67,0.5,0.33,0.17,0.\} \\
 \text{Dominance with small perturbation} & \{0.83,0.5,0.67,0.33,0.17,0.\} \\
 \text{Tree} & \{0.29,0.29,0.29,0.,0.,0.,0.\} \\
 \text{Completely Connected} & \{0.83,0.83,0.83,0.83,0.83,0.83\} \\
 \text{Cyclic} & \{0.17,0.17,0.17,0.17,0.17,0.17\} \\
 \text{Completely Decomposable} & \{0.17,0.17,0.17,0.17,0.17,0.17\} \\
\end{array}
\right)
\]
Comparing the magnitude of each entry in the mean implies a potential ranking for the data.
In fact, for these simple examples, the implied ranking is exactly what we would expect:
$[1,2,3,4,5,6]$ for the Dominance Graph and $[1,3,2,4,5,6]$ for the Dominance graph with small perturbation.
The mean even tells us that the Completely Connected, Cyclic, and Completely Decomposable graphs are made up of objects with equal rank. 
Perhaps the mean can be used as a starting point to get more sophisticated measures of rank and rankability?
\end{remark}

We now consider the centered and covariance matrix associated with our adjacency matrix $A$, which are defined as
\begin{equation}\label{eq: cen}
X = A-\mu e^{T}
\end{equation}
and
\begin{equation}\label{eq: cov}
C_{X} = \frac{1}{n}XX^{T},
\end{equation}
respectively. 

In order to make comparisons to PCA, we consider $X$ to be a data matrix with zero mean.
In particular, we view each row of $X$ as a zero mean measurement of a particular type.
In this case, the $i$th row is a zero mean measurement of which objects are inferior to object $i$. 

Furthermore, the $(i,j)$th element of $C_{X}$ is the dot product between the vector of the $i$th measurement type and the vector of the $j$th measurement type.
We summarize several properties of $C_{X}$ from~\cite{Shlens} below:
\begin{itemize}
\item	$C_{X}$ is a $n\times n$ symmetric matrix.
\item The diagonal entries of $C_{X}$ are the \emph{variance} of particular measurement types.
\item The off-diagonal entries of $C_{X}$ are the \emph{covariance} between measurement types. 
\end{itemize}

\begin{remark}
It is worth noting that the covariance measurements in $C_{X}$ can pick up inconsistencies in the data. 
For instance, the covariance matrix of the Dominance with small perturbation is given below.
\[
\left(
\begin{array}{cccccc}
 0.138889 & 0.0833333 & -0.0555556 & 0.0555556 & 0.0277778 & 0. \\
 0.0833333 & 0.25 & 0.166667 & 0.166667 & 0.0833333 & 0. \\
 -0.0555556 & 0.166667 & 0.222222 & 0.111111 & 0.0555556 & 0. \\
 0.0555556 & 0.166667 & 0.111111 & 0.222222 & 0.111111 & 0. \\
 0.0277778 & 0.0833333 & 0.0555556 & 0.111111 & 0.138889 & 0. \\
 0. & 0. & 0. & 0. & 0. & 0. \\
\end{array}
\right)
\]
Note the negative entry in the $(1,3)$ and $(3,1)$ position of the matrix. 
\end{remark}

The goal of PCA is to minimize redundancy, measured by the magnitude of the covariance, and maximize the signal, measured by the variance.
Since $C_{X}$ is symmetric, the Spectral Theorem implies that $C_{X}$ is orthogonally diagonalizable, see~\cite[Section 7.1]{Lay}~or~\cite[Section 6.6]{Friedberg}.
In addition, $C_{X}$ is positive semidefinite, and it follows that we can diagonalize $C_{X}$ in such a way that its diagonal entries occur in decreasing order. 
Thus, there exists a change of basis $PX=Y$ such that 
\begin{align*}
C_{Y} &= C_{PX} \\
&= \frac{1}{n}(PX)(PX)^{T} \\
&= \frac{1}{n}PXX^{T}P^{T} \\
&= PC_{X}P^{T}
\end{align*}
is a diagonal matrix with non-negative diagonal entries that occur in decreasing order. 

The columns of $P$ correspond to eigenvectors of $C_{X}$ and the diagonal entries of $C_{Y}$ correspond to eigenvalues of $C_{X}$. 
The eigenvectors correspond to directions of maximal variance. However, I am not sure of the connection to ranking at this time.
The eigenvalues, on the other hand, have a clear connection to the rankability of the original dataset. 

\begin{remark}\label{rem: rank_measure}
To have a clearly defined (unique) ranking in a dataset, the variance should be mostly in one direction.
Therefore, when looking at the diagonal entries of $C_{Y}$ one would expect to see the first eigenvalue as (much) larger than the second. 
This is how we define the rankability of a data set:
\[
r=\frac{\lambda_{1}}{\lambda_{2}},
\]
where $\lambda_{1}\geq\lambda_{2}\geq\ldots\geq\lambda_{n}$ are the eigenvalues of $C_{X}$.

Actually, it is not good numerical practice to form the ``normalized'' matrix $XX^{T}$ and then compute its eigenvalues~\cite[Section 5.8]{Watkins} 
Therefore, the related rankability measure may be preferred:
\[
r=\frac{\sigma_{1}}{\sigma_{2}},
\]
where $\sigma_{1}\geq\sigma_{2}\geq\ldots\geq\sigma_{n}$ are the singular values of $X$. 
\end{remark}

Pseudo code for the rankability measure noted in the previous remark is given below. 

\begin{algorithm}[h]
\caption{Rankability Measure}
\begin{algorithmic}
\REQUIRE adjacency matrix $A$
\STATE form the column mean: $\mu=\frac{1}{n}\sum_{j=1}^{n}a_{j}$
\STATE form the centered matrix: $X=A-\mu e^{T}$
\STATE compute the singular values of $X$: $\sigma_{1}\geq\sigma_{2}\geq\ldots\geq\sigma_{n}$
\RETURN $\frac{\sigma_{1}}{\sigma_{2}}$
\end{algorithmic}
\label{alg: rankability}
\end{algorithm}

The rankability measure for each graph given in Figure 5.1 of~\cite{Anderson} is below.

\[
\left(
\begin{array}{cc}
 \text{Dominance Graph} & 1.93185 \\
 \text{Dominance with small perturbation} & 1.64147 \\
 \text{Tree} & 1. \\
 \text{Completely Connected} & 1. \\
 \text{Cyclic} & 1. \\
 \text{Completely Decomposable} & 1. \\
\end{array}
\right)
\]

\begin{theorem}\label{thm: perm_inv}
Let $A$ denote any adjacency matrix. 
Then the rankability measure of $A$ as defined in Remark~\ref{rem: rank_measure} is invariant under permutation similarity. 
\end{theorem}
\begin{proof}
Let $P$ be any permutation matrix and denote by $X$ the centered matrix of $A$ and by $X_{P}$ the centered matrix of $P^{T}AP$. 
Then it follows that
\begin{align*}
X_{P} &= P^{T}AP\left(I-ee^{T}/n\right) \\
&= P^{T}A\left(P-Pee^{T}/n\right) \\
&= P^{T}A\left(I-ee^{T}/n\right)P \\
&= PXP^{T}. 
\end{align*}
Therefore, the singular values of $X_{P}$ and $X$ are the same. 
\end{proof}

The importance of Theorem~\ref{thm: perm_inv} is clear as it guarantees that the rankability of a dataset cannot change by reordering its objects. 
Perhaps under certain constraints this ranking is maximized for dominance graphs?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                 	   			Bibliography
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{Bibliography}
\bibliographystyle{siam}
\bibliography{Bibliography}

\end{document}
